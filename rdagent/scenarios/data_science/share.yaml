describe: # some template to describe some object
  # exp is a template used fo
  exp: |-
    ## {{ heading | default('Best solution of previous exploration of the scenario') }}
    {% if exp %}
    ### Code
    Here is the complete code of the solution.
    {{ exp.experiment_workspace.all_codes }}

    {% if exp.hypothesis is not none %}
    ### Hypothesis for the experiment
    the experiment is designed based on hypothesis: {{exp.hypothesis}}
    {% endif %}

    ### Results
    {% if exp.result is none %}
    There are no according evaluation results
    {% else %}
    Evaluated results on validation are:
    {{ exp.result }}
    {% if exp.format_check_result is not none %}
    Submission format check result is:
    {{ exp.format_check_result }}
    {% endif %}
    {% if exp.running_info.running_time is not none %}
    Running time: {{ exp.running_info.running_time }} seconds
    {% endif %}
    {% endif %}

    {% else %}
    No previous complete experiment available.
    {% endif %}

  feedback: |-
    {% if exp_and_feedback and exp_and_feedback|length > 1 %}
    ## {{heading | default('Previous trial and feedback')}}
    {% if exp_and_feedback[0].hypothesis %}
    The experiment is designed based on hypothesis: {{ exp_and_feedback[0].hypothesis }}
    {% endif %}
    Feedback decision: {{ exp_and_feedback[1].decision }}
    {% if exp_and_feedback[1].code_change_summary  %}Code change summary: {{ exp_and_feedback[1].code_change_summary }}{% endif %}
    Reason: {{ exp_and_feedback[1].reason }}
    {% endif %}

  trace: |-
    {% if exp_and_feedback_list|length == 0 %}
    No previous {% if type == "success" %}SOTA{% elif type == "failure" %}failed{% endif %} experiments available.
    {% else %}
    {% for exp_and_feedback in exp_and_feedback_list %}
    ## Experiment Index: {{ loop.index }}
    Target Problem: {{ exp_and_feedback[0].hypothesis.problem_desc }}
    {% if not pipeline %}Chosen Component: {{ exp_and_feedback[0].hypothesis.component }}{% endif %}
    Proposed Hypothesis: {{ exp_and_feedback[0].hypothesis.hypothesis }}
    {% if exp_and_feedback[1].code_change_summary  %}Code Change Summary: {{ exp_and_feedback[1].code_change_summary }}{% endif %}
    Surpass Previous SOTA: {{ exp_and_feedback[1].decision }}
    {% if exp_and_feedback[0].result is none %}
    Experiment Score: Running buggy
    Experiment Error: {{ exp_and_feedback[1].reason }}
    {% else %}
    Experiment Score: {{ exp_and_feedback[0].result.loc["ensemble"].iloc[0] }}
    Experiment Feedback: {{ exp_and_feedback[1].reason }}
    {% endif %}
    {% endfor %}
    {% endif %}

scen:  # customizable
  role: |-
    You are a Kaggle Grandmaster and expert ML engineer with deep expertise in statistics, machine learning, and competition optimization.
  input_path: "./workspace_input/"
  cache_path: "./workspace_cache/"

component_description:
  DataLoadSpec: |-
    Loads raw competition data, ensuring proper data types, and providing an exploratory data analysis summary.
    - When focusing on this component, the corresponding editing files will be: "load_data.py"
  FeatureEng: |-
    Transforms raw data into meaningful features while maintaining shape consistency, avoiding data leakage, and optimizing for model performance.
    It should be model-agnostic (data transformations/augmentations that apply only to specific model frameworks should not be included here).
    Ensure that any changes you suggest for feature engineering can be implemented without altering the model's code. If the changes require modifications to the model's code, they are considered specific to the model. We should focus on the model component to apply these changes.
    - When focusing on this component, the corresponding editing files will be: "feature.py"
  Model: |-
    Perform one of three tasks: model building, which develops a model to address the problem; model tuning, which optimizes an existing model for better performance; or model removal, which discards models that do not contribute effectively.
    Handle data operations or augmentations that are
    1) closely tied to the model framework, such as tools (e.g., PyTorch's Datasets & DataLoaders) provided by PyTorch or TensorFlow.
    2) cannot be applied in feature engineering ("feature.py") without modifying the model code.
    - When focusing on this component, the corresponding editing files will be: "model_*.py"
  Ensemble: |-
    Combines predictions from multiple models using ensemble strategies, evaluates their performance, and generates the final test predictions.
    - When focusing on this component, the corresponding editing files will be: "ensemble.py"
  Workflow: |-
    Integrates all pipeline components, from data loading to ensemble prediction, ensuring efficient execution and correct output formatting.
    - When focusing on this component, the corresponding editing files will be: "main.py"

component_description_in_pipeline: |-
  [DataLoadSpec]: Focus on the data loading and preprocessing aspects of the pipeline, ensuring that the data is correctly formatted and ready for feature engineering.
  [FeatureEng]: Concentrate on transforming the raw data into meaningful features while maintaining the integrity of the dataset.
  [Model]: Focus on the model building, tuning of the pipeline, ensuring that the model is optimized for performance.
  [Ensemble]: Concentrate on combining predictions from multiple models and evaluating their performance.
  [Workflow]: Focus on the overall integration of the pipeline or parts not included in the other components, ensuring that all components work together seamlessly.

component_spec:
  general: |-
    {{ spec }}

    Your code will be tested by the code below. You must ensure your implementation passes the test code:
    ```python
    {{ test_code }}
    ```
  DataLoadSpec: |-
    1. File Handling:
      - Handle file encoding and delimiters appropriately.
      - Combine or process multiple files if necessary.
      - Avoid using the sample submission file to infer test indices. If a dedicated test index file is available, use that. If not, use the order in the test file as the test index.
      - If each prediction sample is linked to a file on disk, simply load the file paths (please load the full path to make it easier to write the loader in following workflows) as X/features without any additional processing.

    2. Data Preprocessing:
      - Convert data types correctly (e.g., numeric, categorical, date parsing).
      - Optimize memory usage for large datasets using techniques like downcasting or reading data in chunks if necessary.
      - Domain-Specific Handling: 
        - Apply competition-specific preprocessing steps as needed (e.g., text tokenization, image resizing).
        - Instead of returning binary bytes directly, convert/decode them into more useful formats like numpy.ndarrays.

    3. Code Standards:
      - DO NOT use progress bars (e.g., `tqdm`).
      - DO NOT use the sample submission file to extract test index information.
      - DO NOT exclude features inadvertently during this process.

    4. Exploratory Data Analysis (EDA) [Required]:
      - Before returning the data, you should always add an EDA part describing the data to help the following steps understand the data better.
      - The EDA part should be drafted in plain text with certain format schema with no more than ten thousand characters.
      - An evaluation agent will help to check whether the EDA part is added correctly.

    5. NOTES
      - Never use sample submission as the test index, as it may not be the same as the test data. Use the test index file or test data source to get the test index.

  FeatureEng: |-
    1. Well handle the shape of the data
      - The sample size of the train data and the test data should be the same in all scenarios.
      - To some tabular or time-series data, you may add or remove some columns so your inferred column number may be unsure.
      - For scenarios where each dimension does not have a special meaning (like image, audio, and so on), the input shape and the output shape should be exactly the same in most cases unless there is a compelling reason to change them.

    2. Integration with the Model Pipeline:
      - If feature engineering is deferred to the model pipeline for better overall performance, state explicitly that it will be handled at the model stage.
        - Model-related operations should not be implemented in this step. (e.g., it uses tools combined with models like torch.Dataset with rich data transformation/augmentation)
      - Otherwise, ensure this function applies all required transformations while avoiding data leakage.

    3. General Considerations:
      - Ensure scalability for large datasets.
      - Handle missing values and outliers appropriately (e.g., impute, remove, or replace).
      - Ensure consistency between feature data types and transformations.
      - Prevent data leakage: Do not use information derived from the test set when transforming training data.

    4. Code Standards:
      - Avoid using progress bars (e.g., `tqdm`) in the implementation.          

    5. Notes:
      - GPU and multiprocessing are available and are encouraged to use for accelerating transformations.
      - Feature engineering should be executed **once** and reused across all models to ensure consistency: `X_transformed, y_transformed, X_test_transformed = feat_eng(X, y, X_test)`
      - If the data loader returns the file path directly, we can skip feature engineering and return original values directly.
  
  Model: |-
    - Do not use progress bars (e.g., `tqdm`) in the implementation.
    - The device has GPU support, so you are encouraged to use it for training if necessary to accelerate the process.
    - Some data transformations/augmentations can be included in this step (e.g., data tools provided by TensorFlow and Torch)
      - Please correctly handle data transformations/augmentations, especially when the dataloader loads the file path directly.
    - Ensure dynamic handling of feature dimensions to accommodate potential enhancements in input features without requiring code modifications.
  
  Ensemble: |-
    1. Input Validation:
      - Handle empty or invalid inputs gracefully with appropriate error messages.

    2. Metric Calculation and Storage:
      - Calculate the metric (mentioned in the evaluation section of the competition information) for each model and ensemble strategy on valid, and save the results in `scores.csv`, e.g.:
      ```python
      scores = {}
      for model_name, val_pred in val_preds_dict.items():
          scores[model_name] = calculate_metric(val_label, val_pred)
      
      ...
      一些关于集成策略的代码
      ...
      ensemble_val_pred = ...

      ensemble_score = calculate_metric(val_label, ensemble_val_pred)
      scores["ensemble"] = ensemble_score  # 确保 "ensemble" 被明确存储
      
      scores_df = pd.DataFrame(scores.items(), columns=["Model", <metric_name>])
      scores_df.to_csv("scores.csv", index=False)
      ```
      - 即使只有一个模型存在，也要计算集成分数并将其存储在 "ensemble" 下。

    3. 代码标准：
      - 不要在代码中使用进度条（例如 tqdm）。

    4. 注意事项：
      - 确保能够灵活处理基于比赛要求的多种集成策略。

  Workflow: |-
    您的任务是为 Kaggle 风格的机器学习比赛项目实现主工作流程脚本（`main.py`）。
    请遵循提供的项目结构和规范，以确保一致性和可维护性：
    1. 工作流程集成：
      - 将以下组件集成到工作流程中：
        - 数据加载（`load_data.py`）。
        - 特征工程（`feature.py`）。
        - 用于训练和测试的模型工作流程（`model_*.py`）。
        - 集成工作流程，将模型工作流程的结果结合起来以获得最终预测（`ensemble.py`）。
      - 将每个组件视为模块化且可调用的 Python 函数。
      - 工作流程脚本应足够灵活以处理单个模型或多个模型，文件名（model_*.py）在开始时未确定。
        对于多个模型选择，使用 Python 代码根据文件名识别合格模型，例如：
        ```python
        available_models = [f for f in os.listdir('.') if f.startswith('model_') and 'test' not in f]
        ```
      - 工作流程脚本应直接可执行。我们将按原样运行您的脚本，因此不要假设您的函数会被单独导入和调用。
    2. 特征工程
      - 特征工程应仅调用一次。例如：
        `X_transformed, y_transformed, X_test_transformed = feat_eng(X, y, X_test)`
      - 它应该在数据集拆分之前被调用。

    3. 数据集拆分
      - `load_data` 返回的数据集没有预先拆分。在调用 `feat_eng` 之后，将数据拆分为训练集和测试集。
      - [注意] 如果可行，对训练集（`X_transformed`，`y_transformed`）应用交叉验证，以确保可靠评估模型性能。
      - 保持测试集（`X_test_transformed`）不变，因为它仅用于生成最终预测。
      - 参考的伪代码逻辑：
        ```
        设置拆分数量并初始化 KFold 交叉验证器。
        为验证和测试预测创建字典。
        对于每个模型文件：
            动态导入模型。
            初始化用于折叠外（OOF）和测试预测的数组。
            对于 KFold 中的每个折叠：
                将数据拆分为训练集和验证集。
                运行模型工作流程以获取验证和测试预测。
                验证形状。
                存储验证和测试预测。
            计算跨折叠的平均测试预测。
            保存 OOF 和平均测试预测。
        从所有模型中集成预测并打印最终形状。
        ```

    4. 提交文件：
      - 将最终预测保存为 `submission.csv`，确保格式符合比赛要求（请参阅文件描述中的 `sample_submission` 以获取正确的结构）。
      - 明确提交格式的要求，并确保输出符合该格式。

    5. 代码标准：
      - 不要在代码中使用进度条（例如 tqdm）。

    6. 集成策略：
      将所有模型输出合并到一个字典中，其中每个键是模型的文件名（不包括 .py 扩展名），相应的值是模型的输出。
      示例代码：
      {% raw %}
      {% for model_name in model_names %}
      model_module = __import__(model_name.replace('.py', ''))
      val_pred, test_pred, _ = model_module.model_workflow(
        X=train_X,
        y=train_y,
        val_X=val_X,
        val_y=val_y,
        test_X=X_test_transformed
      )
      val_preds_dict[model_module.__name__] = val_pred
      test_preds_dict[model_module.__name__] = test_pred
      {% endfor %}
      final_pred = ensemble_workflow(test_preds_dict, val_preds_dict, val_y)
      {% endraw %}
    
  Pipeline: |-
    0. 程序执行:
      - 通过运行 `python main.py` 来执行工作流程，不带任何命令行参数。确保 `main.py` 不需要或期望任何参数。
      - 工作目录将只包含 `main.py`。执行所需的任何其他文件必须由 `main.py` 本身下载或生成。
      
    1. 文件处理:
      - 适当处理文件编码和分隔符。
      - 如有必要，合并或处理多个文件。
      - 避免使用示例提交文件来推断测试索引。如果有专用的测试索引文件，请使用它。如果没有，请按测试文件中的顺序使用测试索引。
      - 确保从文件中加载实际数据，而不仅仅是文件名或路径。不要推迟数据加载到后面的步骤中。

    2. 数据预处理:
      - 正确转换数据类型（例如，数值型、分类变量、日期解析）。
      - 使用降级或分块读取等技术优化大数据集的内存使用。
      - 特定领域的处理： 
        - 根据需要应用比赛特定的预处理步骤（例如，文本标记化、图像调整大小）。

    3. 代码标准:
      - 不要使用进度条（例如，`tqdm`）。
      - 不要使用示例提交文件提取测试索引信息。
      - 在此过程中不要无意中排除特征。

    4. 注意事项
      - 永远不要将示例提交作为测试索引，因为它可能与测试数据不同。使用测试索引文件或测试数据源获取测试索引。
      - 对于神经网络模型，如果可能，尽量使用 pytorch 而不是 tensorflow 作为后端。
      - 对于决策树模型，如果可能，尽量使用 xgboost 或 RandomForest 而不是 lightgbm 作为后端。
      - 对于神经网络模型，首先尝试从预训练模型开始，然后进行微调，而不是从头开始训练，通常会得到更好的效果。

    5. 一般注意事项:
      - 确保对大数据集的可扩展性。
      - 适当处理缺失值和异常值（例如，插补、删除或替换）。
      - 确保特征数据类型和转换的一致性。
      - 防止数据泄漏：在转换训练数据时，不要使用从测试集派生的信息。

    6. 注意事项:
      - 提供 GPU 和多处理支持，鼓励加速转换。
  
    7. 指标计算和存储:
      - 计算每个模型和集成策略在有效数据上的指标（在比赛信息的评估部分提到），并将结果保存在 `scores.csv` 中
      - 评估应基于 5 倍交叉验证，但仅在这对手头任务是适当的评估时进行。在每个模型上将 5 倍交叉验证的平均验证分数存储在 `scores.csv` 中。
      - 即使只有一个模型存在，也要计算集成分数并将其存储在 "ensemble" 下。
      - `scores.csv` 的索引应包括模型名称和 "ensemble" 策略。"ensemble" 应该完全用小写字母表示。集成是来自多个模型的结果。如果只存在一个模型，则集成分数应与模型分数相同。
      - `scores.csv` 中的列名应为:
        - Model: 模型或集成策略的名称。
        - <metric_name>: 该模型或集成策略的计算指标值。指标名称可以在场景描述中找到。指标名称应与场景描述中的名称完全相同，因为用户将用它来检查结果。
      - 验证指标应在所有想法和实现中保持一致。避免提出可能影响验证指标的想法并修改相关代码。

    8. 提交文件:
      - 将最终预测保存为 `submission.csv`，确保格式符合比赛要求（请参阅文件描述中的 `sample_submission` 以获取正确的结构）。
      - 明确提交格式的要求，并确保输出符合该格式。

guidelines:
  coding: |-
    You might receive exploratory data analysis (EDA) details about the source data. Do not use this EDA information to create assertions or raise errors. We might generate sample data for quick coding (so your code may run on sample data which is part of the full-size data), but remember that the EDA details are based on the full-size data.
